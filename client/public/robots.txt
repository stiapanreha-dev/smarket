# robots.txt for SnailMarketplace

# Allow all search engines to crawl public pages
User-agent: *

# Allow crawling of main pages
Allow: /
Allow: /catalog
Allow: /catalog/*
Allow: /search

# Disallow private/protected pages
Disallow: /merchant
Disallow: /merchant/*
Disallow: /checkout
Disallow: /checkout/*
Disallow: /profile
Disallow: /profile/*
Disallow: /orders
Disallow: /orders/*
Disallow: /cart
Disallow: /wishlist
Disallow: /notifications

# Disallow auth pages (no need for search engines to index these)
Disallow: /login
Disallow: /register

# Disallow API endpoints
Disallow: /api/*

# Crawl delay (be nice to search engines)
Crawl-delay: 1

# Sitemap location
Sitemap: https://snailmarketplace.com/sitemap.xml
Sitemap: https://snailmarketplace.com/sitemap-products.xml
